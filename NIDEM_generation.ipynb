{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# National Intertidal Digital Elevation Model (NIDEM)\n",
    "\n",
    "**What does this notebook do?** This notebook uses the ITEM 2.0 product to compute continuous elevation data for Australia's intertidal zone. It initially imports ITEM REL files and median tidal elevations for each tidal interval, computes elevations at interval boundaries, extracts contours around each tidal interval, and then interpolates between these contours using TIN/Delaunay triangulation linear interpolation. This interpolation method preserves the tidal interval boundaries of ITEM 2.0. The notebook exports the resulting DEM as a geotiff, the contours used for interpolation as line shapefiles, and a mask geotiff with tags indicating locations of nodata values (== 1), the lowest ITEM interval (2), the highest ITEM interval (3), and ITEM confidence NDWI standard deviation greater than 0.25 (4).\n",
    "\n",
    "**Date:** April 2018\n",
    "\n",
    "**Author:** Robbi Bishop-Taylor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Tags: :index:`ITEM`, :index:`interpolation`, :index:`digital_elevation_model`, :index:`intertidal_zone`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import fiona\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import statsmodels.formula.api as sm\n",
    "from collections import OrderedDict\n",
    "from shapely.geometry import Point, LineString, MultiLineString, mapping\n",
    "from pyproj import Proj, transform\n",
    "from fiona.crs import from_epsg\n",
    "from osgeo import gdal\n",
    "from scipy.interpolate import griddata\n",
    "from scipy import ndimage as nd\n",
    "from scipy import stats\n",
    "from skimage import measure\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Import DEA Notebooks scripts\n",
    "sys.path.append(os.path.abspath('/g/data/r78/rt1527/dea-notebooks/algorithms'))\n",
    "import SpatialTools \n",
    "\n",
    "\n",
    "def fill(data, invalid=None):\n",
    "    \"\"\"\n",
    "    Replace value of invalid cells by the value of the nearest \n",
    "    cell with valid data\n",
    "\n",
    "    :attr data: numpy array of any dimension\n",
    "    :attr invalid: a binary array of same shape as 'data'. Data value \n",
    "                   are replaced where invalid is True. Defaults to \n",
    "                   np.isnan(data) if no layer is given\n",
    "\n",
    "    :returns: Array with invalid values filled with nearest valid data value\n",
    "    \"\"\"    \n",
    "    \n",
    "    # If no invalid array is given, default to setting invalid based on nan\n",
    "    if invalid is None:\n",
    "        \n",
    "        invalid = np.isnan(data)\n",
    "\n",
    "    ind = nd.distance_transform_edt(invalid, \n",
    "                                    return_distances=False, \n",
    "                                    return_indices=True)\n",
    "    \n",
    "    return data[tuple(ind)]\n",
    "\n",
    "    \n",
    "def reproject_to_template(input_raster, template_raster, output_raster, resolution=None,\n",
    "                         resampling=gdal.GRA_Average, nodata_val=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reprojects a raster to the extent, cell size, projection and dimensions of a template \n",
    "    raster using GDAL. Optionally, can set custom resolution for output reprojected raster\n",
    "    using 'resolution'; this will affect raster dimensions/width/columns.\n",
    "    \n",
    "    Last modified: April 2018\n",
    "    Author: Robbi Bishop-Taylor    \n",
    "    \n",
    "    :attr input_raster: path to input geotiff raster to be reprojected (.tif)\n",
    "    :attr template_raster: path to template geotiff raster (.tif) used to copy extent, projection etc\n",
    "    :attr output_raster: output reprojected raster path with geotiff extension (.tif)\n",
    "    :attr resolution: optionally set custom cell size for output reprojected raster; defaults to \n",
    "                      'None', or the cell size of template raster \n",
    "    :attr resampling: GDAL resampling method to use for reprojection; defaults to gdal.GRA_Average \n",
    "    :attr nodata_val: values in the output reprojected raster to set to nodata; defaults to 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import raster to reproject\n",
    "    print(\"Importing raster datasets\")\n",
    "    input_ds = gdal.Open(input_raster)\n",
    "    input_proj = input_ds.GetProjection()\n",
    "    input_geotrans = input_ds.GetGeoTransform()\n",
    "    data_type = input_ds.GetRasterBand(1).DataType\n",
    "    n_bands = input_ds.RasterCount  \n",
    "    \n",
    "    # Import raster to use as template\n",
    "    template_ds = gdal.Open(template_raster)   \n",
    "    template_proj = template_ds.GetProjection()\n",
    "    template_geotrans = template_ds.GetGeoTransform()\n",
    "    template_w = template_ds.RasterXSize\n",
    "    template_h = template_ds.RasterYSize\n",
    "    \n",
    "    # Use custom resolution if supplied\n",
    "    if resolution:\n",
    "        \n",
    "        template_geotrans[1] = float(resolution)\n",
    "        template_geotrans[-1] = -float(resolution)\n",
    "\n",
    "    # Create new output dataset to reproject into\n",
    "    output_ds = gdal.GetDriverByName('Gtiff').Create(output_raster, template_w, \n",
    "                                                     template_h, n_bands, data_type)  \n",
    "    output_ds.SetGeoTransform(template_geotrans)\n",
    "    output_ds.SetProjection(template_proj)\n",
    "    output_ds.GetRasterBand(1).SetNoDataValue(nodata_val)\n",
    "\n",
    "    # Reproject raster into output dataset\n",
    "    print(\"Reprojecting raster\")\n",
    "    gdal.ReprojectImage(input_ds, output_ds, input_proj, template_proj, resampling)\n",
    "    \n",
    "    # Close datasets\n",
    "    input_ds = None\n",
    "    template_ds = None    \n",
    "    \n",
    "    print(\"Reprojected raster exported to {}\".format(output_raster))\n",
    "    return output_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up analysis\n",
    "Set up path to data and polygon to process. `plotting_subset` does not affect the analysis, but can be used to provide more useful zoomed-in plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory\n",
    "os.chdir(\"/g/data/r78/rt1527/nidem\")\n",
    "\n",
    "# Path to ITEM offset product\n",
    "item_offset_path = \"/g/data2/v10/ITEM/offset_products\"\n",
    "item_relative_path = \"/g/data2/v10/ITEM/rel_products\"\n",
    "item_conf_path = \"/g/data2/v10/ITEM/conf_products\"\n",
    "\n",
    "# Set ITEM polygon for analysis\n",
    "polygon_ID = 252\n",
    "\n",
    "# Zoom-in for plots (has no effect on analysis but allows you to plot a zoomed-in subset)\n",
    "plotting_subset = np.index_exp[:, :]\n",
    "plotting_subset = np.index_exp[2000: 2400, 1500:1750]\n",
    "\n",
    "# Print run details\n",
    "print(\"Processing polygon {0} from {1}\".format(polygon_ID, item_offset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ITEM interval boundary value extraction\n",
    "ITEM offset values represent the median tidal height for each tidal interval ([Sagar et al. 2015](https://doi.org/10.1016/j.rse.2017.04.009)). Because ITEM tidal intervals are linearly spaced by design, this code uses a simple linear model to compute new offset values for each interval boundary (e.g. the boundary between ITEM interval 1 and 2). This allows us to assign a more appropriate tidal height to the contours that divide the ITEM tidal intervals than would be possible through simply assigning median tidal heights to the downhill or uphill contours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import ITEM offset values for each ITEM tidal interval\n",
    "item_offsets = np.loadtxt(\"{}/elevation.txt\".format(item_offset_path), delimiter = \",\", dtype = \"str\")\n",
    "item_offsets = {int(key):[int(val) for val in value.split(\" \")] for (key, value) in item_offsets}\n",
    "interval_offsets = item_offsets[polygon_ID]\n",
    "\n",
    "# Create dataframe of offset values by ITEM interval\n",
    "interval_offsets_df = pd.DataFrame({\"item_interval\": range(1,10), \"offset\": interval_offsets})\n",
    "display(interval_offsets_df.set_index(\"item_interval\"))\n",
    "\n",
    "# Compute linear model and calculate ITEM offsets at the boundary of each ITEM interval \n",
    "# (ensures that extracted contours are placed precisely on the boundary of ITEM intervals)\n",
    "m, b = np.polyfit(interval_offsets_df[\"item_interval\"], interval_offsets_df[\"offset\"], 1)\n",
    "interval_boundaries = np.arange(0.5, 10.5, 1.0)\n",
    "contour_offsets = (m * interval_boundaries + b).astype(int)\n",
    "\n",
    "# Compute ITEM offset interval used to fill lowest class of ITEM relative layer \n",
    "# (not used for interpolation, but ensures lowest contour is placed exactly on interval boundary)\n",
    "interval_zero = (m * 0 + b).astype(int)\n",
    "\n",
    "# Plot output\n",
    "fig, ax = plt.subplots(figsize = (8, 5))\n",
    "interval_offsets_df.plot(kind = \"scatter\", x = \"item_interval\", y = \"offset\", s = 40, \n",
    "                         color = \"black\", xticks=interval_offsets_df['item_interval'], ax = ax)\n",
    "ax.scatter(interval_boundaries, contour_offsets, color = 'red', marker = \"x\")\n",
    "ax.plot(interval_boundaries, contour_offsets, color = 'red', linestyle = '--', lw = 0.5)\n",
    "for xc in interval_boundaries:\n",
    "    plt.axvline(x = xc, color = 'k', linestyle = '--', lw = 0.5)\n",
    "for i, txt in enumerate(contour_offsets):\n",
    "    ax.annotate(txt, (interval_boundaries[i] - 0.3, contour_offsets[i] + 150), color = \"red\")\n",
    "plt.title(\"ITEM OFFSET interval boundaries\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and prepare ITEM offset raster\n",
    "Imports ITEM REL raster for given polygon, and use a lookup index array of offset values to classify into a new array of evenly-spaced ITEM offset values (in *mm* units relative to sea level) suitable for contour extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raster and extract shape, projection info and geotransform data\n",
    "item_filename = glob.glob(\"{}/ITEM_REL_{}_*.tif\".format(item_relative_path, polygon_ID))[0]\n",
    "item_ds = gdal.Open(item_filename)\n",
    "item_array = item_ds.GetRasterBand(1).ReadAsArray() \n",
    "yrows, xcols = item_array.shape\n",
    "prj = item_ds.GetProjection()\n",
    "geotrans = item_ds.GetGeoTransform()\n",
    "upleft_x, x_size, x_rotation, upleft_y, y_rotation, y_size = geotrans\n",
    "\n",
    "# Temporarily assign nodata -6666 values to new class 10 prior to lookup classification\n",
    "item_array[item_array == -6666] = 10\n",
    " \n",
    "# Create lookup index array, and index by ITEM relative layer to classify into offset values\n",
    "# (this method should be resilient to ITEM layers with fewer than 9 classes)\n",
    "test_lut = np.array([interval_zero] + interval_offsets + [np.nan])\n",
    "offset_array = test_lut[item_array]\n",
    "\n",
    "# Dilate data area of array by two pixels to eliminate NA contours on tile edges\n",
    "# (this only affects pixels directly on the boundary of two polygon arrays)\n",
    "dilated_array = nd.morphology.binary_dilation(~np.isnan(offset_array),  iterations = 2)\n",
    "offset_array = fill(offset_array)\n",
    "offset_array[~dilated_array] = np.nan\n",
    "\n",
    "# Plot original data\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.imshow(offset_array[plotting_subset])  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract contours\n",
    "Uses `scikit.measure.find_contours` to rapidly extract contour boundaries between ITEM tidal intervals, and assigns these contours with previously calculated elevation values. Contours are exported as line shapefiles to assist in subsequent assessment of output DEMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output dict to hold contours for each offset\n",
    "contour_dict = OrderedDict()\n",
    "\n",
    "try:\n",
    "    for contour_offset in contour_offsets:\n",
    "\n",
    "        # Extract contours from array\n",
    "        contours = measure.find_contours(offset_array, contour_offset)      \n",
    "        print(\"Extracting contour {}\".format(contour_offset))\n",
    "        \n",
    "        # Iterate through each contour feature, remove NAs and fix coordinates\n",
    "        contour_list = list()\n",
    "        for contour in contours:\n",
    "\n",
    "            # Convert index coordinates to spatial coordinates in-place\n",
    "            contour[:, 0] = contour[:, 0] * float(y_size) + upleft_y + (float(y_size) / 2)\n",
    "            contour[:, 1] = contour[:, 1] * float(x_size) + upleft_x + (float(x_size) / 2)        \n",
    "            contour = np.insert(contour, 2, contour_offset, axis = 1)\n",
    "\n",
    "            # Remove contour points with NAs\n",
    "            contour = contour[~np.isnan(contour).any(axis=1)] \n",
    "            contour_list.append(contour)\n",
    "\n",
    "        # Add list of contour arrays to dict \n",
    "        contour_dict[contour_offset] = contour_list\n",
    "except:\n",
    "    print(\"fail\")\n",
    "\n",
    "    \n",
    "# Export contours to line shapefile to assist in evaluating DEMs\n",
    "schema = {'geometry':  'MultiLineString', \n",
    "          'properties': { 'elevation': 'int' } }      \n",
    "\n",
    "with fiona.open(\"output_data/contour/NIDEM_contours_{}.shp\".format(polygon_ID), \"w\", \n",
    "                crs = from_epsg(3577),\n",
    "                driver = \"ESRI Shapefile\", \n",
    "                schema = schema) as output:\n",
    "    \n",
    "    for elevation_value, contour_list in contour_dict.items():\n",
    "        \n",
    "        # Filter out contours with less than two points (i.e. non-lines)\n",
    "        contour_list = [x for x in contour_list if len(x) > 1]\n",
    "        \n",
    "        # Create multiline string by first flipping coordinates then creating list of linestrings\n",
    "        contour_linestrings = [LineString([(x, y) for (y, x, z) in contour_array]) \n",
    "                               for contour_array in contour_list]\n",
    "        contour_multilinestring = MultiLineString(contour_linestrings)\n",
    "\n",
    "        # Write output shapefile to file with elevation field        \n",
    "        output.write({'properties': { 'elevation': int(elevation_value) },\n",
    "                      'geometry': mapping(contour_multilinestring) })\n",
    "        \n",
    "# Chain and concatenate all arrays nested within array lists (i.e. individual collections of same \n",
    "# elevation contours) and dictionary entries (i.e. collections of all same-elevation contours)\n",
    "all_contours = np.concatenate(list(itertools.chain.from_iterable(contour_dict.values())))\n",
    "points = all_contours[:, 0:2]\n",
    "values = all_contours[:, 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate contours using TIN/Delaunay triangulation interpolation\n",
    "Exports a DEM by interpolating previously extracted contours. This uses the linear method from `scipy.interpolate.griddata`, which computes a TIN/Delaunay triangulation of the input data using Qhull before performing linear barycentric interpolation on each triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bounds of ITEM layer\n",
    "minx = upleft_x\n",
    "maxx = upleft_x + (x_size * xcols)\n",
    "miny = upleft_y + (y_size * yrows)\n",
    "maxy = upleft_y\n",
    "\n",
    "# Create interpolation grid (from, to, by values in metre units)\n",
    "grid_y, grid_x = np.mgrid[maxy:miny:1j * yrows,\n",
    "                          minx:maxx:1j * xcols]\n",
    "\n",
    "# Interpolate between points onto grid. This uses the 'linear' method from \n",
    "# scipy.interpolate.griddata, which computes a TIN/Delaunay triangulation of the input \n",
    "# data with Qhull and performs linear barycentric interpolation on each triangle\n",
    "print(\"Interpolating data for polygon {}\".format(polygon_ID))\n",
    "interpolated_array = griddata(points, values, (grid_y, grid_x), method = \"linear\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask interpolated array to keep only good data\n",
    "Masks the interpolated array to remove areas with nodata, and the lowest and highest elevation intervals from the original ITEM layer (lowest and highest intervals cannot be correctly interpolated as they have no lower or upper bounds). In addition, masks out areas with high ITEM confidence NDWI standard deviation (i.e. areas where inundation patterns are not driven by tidal influences), as elevation offset values in these areas are likely to be invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ITEM confidence NDWI standard deviation array for polygon\n",
    "conf_filename = glob.glob(\"{}/ITEM_STD_{}_*.tif\".format(item_conf_path, polygon_ID))[0]\n",
    "conf_ds = gdal.Open(conf_filename)\n",
    "conf_array = conf_ds.GetRasterBand(1).ReadAsArray() \n",
    "\n",
    "# Create mask flagging nodata values (== 1), lowest ITEM interval (2), highest ITEM \n",
    "# interval (3), and ITEM confidence NDWI standard deviation greater than 0.25 (4)\n",
    "baddata_mask = np.full(item_array.shape, np.nan)\n",
    "baddata_mask[item_array == 10] = 1  # areas with nodata in original ITEM layer\n",
    "baddata_mask[item_array == 0] = 2  # areas in lowest ITEM interval\n",
    "baddata_mask[item_array == 9] = 3  # areas in highest ITEM interval\n",
    "baddata_mask[conf_array > 0.25] = 4  # areas with > 0.25 ITEM confidence NDWI STD\n",
    "\n",
    "# Mask DEM to keep only data with no bad data flags\n",
    "print(\"Masking data for polygon {}\".format(polygon_ID))\n",
    "interpolated_array[~np.isnan(baddata_mask)] = np.nan\n",
    "\n",
    "# Plot output interpolated and masked DEM\n",
    "plt.figure(figsize = (9, 9))\n",
    "plt.imshow(interpolated_array[plotting_subset])\n",
    "plt.show()\n",
    "\n",
    "# Export resulting DEM as a geotiff\n",
    "print(\"Exporting DEM for polygon {}\".format(polygon_ID))\n",
    "SpatialTools.array_to_geotiff(fname = \"output_data/dem/NIDEM_dem_{}.tif\".format(polygon_ID), \n",
    "                              data = interpolated_array, \n",
    "                              geo_transform = geotrans, \n",
    "                              projection = prj, \n",
    "                              nodata_val = np.nan)\n",
    "\n",
    "# Export mask to geotiff\n",
    "SpatialTools.array_to_geotiff(fname = \"output_data/mask/NIDEM_mask_{}.tif\".format(polygon_ID), \n",
    "                              data = baddata_mask, \n",
    "                              geo_transform = geotrans, \n",
    "                              projection = prj, \n",
    "                              nodata_val = np.nan,\n",
    "                              dtype=gdal.GDT_Byte)\n",
    "\n",
    "# Close files\n",
    "conf_ds = None\n",
    "item_ds = None\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
